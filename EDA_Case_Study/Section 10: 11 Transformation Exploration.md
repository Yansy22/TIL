1. 📊 정량적 분석 (Statistical Analysis)
코드의 첫 번째 부분(distribution_stats 생성)은 각 피처의 분포를 **"숫자"**로 요약합니다.

stats.normaltest(train_df[feature])

목적: "이 피처가 정규 분포인가?"라는 질문에 대한 통계적 검정입니다.

Normality_p_value: 이것이 핵심입니다.

p-value > 0.05: "정규 분포와 비슷하다." (Is_Normal: True)

p-value < 0.05: "정규 분포와 다르다." (Is_Normal: False)

skew() (왜도)

목적: "데이터가 얼마나 비대칭인가?"

Skewness ≈ 0: 좌우 대칭 (정규 분포)

Skewness > 0 (양수): 오른쪽 꼬리가 길다. (예: 소득 분포. 대부분은 적게 벌고, 일부 고소득자가 꼬리를 길게 만듦)

Skewness < 0 (음수): 왼쪽 꼬리가 길다.

kurtosis() (첨도)

목적: "데이터가 얼마나 뾰족한가?" (정규 분포 대비)

Kurtosis ≈ 0: 적당한 뾰족함 (정규 분포)

Kurtosis > 0 (양수): 더 뾰족하다. (데이터가 평균에 더 몰려있고, 꼬리가 두꺼움. "이상치"가 많다는 신호)

Kurtosis < 0 (음수): 더 펑퍼짐하다.

2. 📈 시각적 진단 (Visual Diagnosis)
코드의 두 번째 부분(6개 서브플롯)은 1번의 '숫자'들을 **"그림"**으로 확인하는 과정입니다.

axes[idx].hist(data, ...)

목적: 데이터의 실제 분포(파란색 막대)를 그립니다.

stats.norm.pdf(x, data.mean(), data.std())

목적: "만약 이 데이터가 완벽한 정규 분포였다면" 이런 모양이었을 것이라는 **가상의 '종 모양'(빨간색 선)**을 겹쳐 그립니다.

axes[idx].set_title(...)

그래프 제목에 1번에서 계산한 왜도(Skew)와 첨도(Kurtosis) 값을 함께 표시합니다.

💡 EDA 최종 결론 및 다음 단계
이 두 분석을 통해 분석가는 **"파란색 막대(실제)가 빨간색 선(이상)과 얼마나 다른가?"**를 한눈에 파악합니다.

만약 Is_Normal: False가 많고, 히스토그램이 빨간 선과 다르다면 (대부분의 실제 데이터가 이렇습니다):

"이 피처들은 정규 분포 가정을 위반한다. 특히 age는 오른쪽 꼬리가 길다(Skew > 0)."

그래서 다음 단계는? (Actionable Insight)

"모델 성능을 높이기 위해, age처럼 오른쪽으로 쏠린 피처는 **로그 변환(np.log1p)**이나 **제곱근 변환(np.sqrt)**을 적용하여 정규 분포에 가깝게 '펴주는' 변수 변환이 필요하다."

(이것이 바로 Section 8의 3번 '변수 변환 테스트' 코드가 존재했던 이유입니다.)
