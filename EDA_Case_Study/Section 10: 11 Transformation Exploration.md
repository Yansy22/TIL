# Section 10. ë³€ìˆ˜ ë³€í™˜ íƒìƒ‰ (Transformation Exploration)

ì´ ì„¹ì…˜ì˜ ëª©ì ì€ "ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´, ìˆ˜ì¹˜í˜• í”¼ì²˜(ë°ì´í„°)ë¥¼ ì–´ë–¤ 'ëª¨ì–‘'ìœ¼ë¡œ ê°€ê³µ(ë³€í™˜)í•˜ëŠ” ê²ƒì´ ê°€ì¥ ìµœì ì¸ê°€?"ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**í•µì‹¬ ì „ì œ:** ë§ì€ **ì„ í˜• ëª¨ë¸(Logistic Regression ë“±)** ì€ ë°ì´í„°ê°€ **ì •ê·œ ë¶„í¬(ì¢… ëª¨ì–‘)** ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ í•œìª½ìœ¼ë¡œ ì‹¬í•˜ê²Œ ì ë ¤ìˆìœ¼ë©´(Skewed), ëª¨ë¸ì´ ë¶ˆì•ˆì •í•´ì§€ê³  ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ì„¹ì…˜ì€ "ì§„ë‹¨ â†’ ì¹˜ë£Œ â†’ ê²€ì¦"ì˜ 5ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

---

## 1ë‹¨ê³„: ğŸ©º ì§„ë‹¨ (Analyze original distributions)

* **ëª©ì :** "í˜„ì¬ ë°ì´í„°ì˜ ê±´ê°• ìƒíƒœ(ë¶„í¬)ëŠ” ì–´ë– í•œê°€?"ë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤.
* **ë°©ë²•:** `stats.normaltest` (p-value), `skew` (ì™œë„/ì ë¦¼), `kurt` (ì²¨ë„/ë¾°ì¡±í•¨)ë¥¼ **ìˆ«ì**ë¡œ ê³„ì‚°í•˜ê³ , íˆìŠ¤í† ê·¸ë¨(ì‹¤ì œ)ê³¼ ì •ê·œë¶„í¬(ì´ìƒ)ë¥¼ **ê·¸ë¦¼**ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.
* **ì½”ë“œ ì˜ˆì‹œ:**
    ```python
    # 1. ìˆ«ìë¡œ ì§„ë‹¨ (ì •ê·œì„± ê²€ì •)
    for feature in numeric_features:
        statistic, p_value = stats.normaltest(train_df[feature])
        skew = train_df[feature].skew()
        # ... (distribution_stats DataFrameì— ì €ì¥) ...
    print(distribution_stats.round(4))

    # 2. ê·¸ë¦¼ìœ¼ë¡œ ì§„ë‹¨ (ì‹œê°í™”)
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    for idx, feature in enumerate(numeric_features):
        data = train_df[feature]
        # ì‹¤ì œ ë¶„í¬ (íŒŒë€ìƒ‰ ë§‰ëŒ€)
        axes[idx].hist(data, bins=30, density=True, alpha=0.7, color='skyblue')
        # ì´ìƒì ì¸ ì •ê·œë¶„í¬ (ë¹¨ê°„ìƒ‰ ì„ )
        axes[idx].plot(x, stats.norm.pdf(x, data.mean(), data.std()), 'r-', label='Normal')
    ```
* **ğŸ’¡ ì¸ì‚¬ì´íŠ¸:**
    * `Is_Normal: False`ê°€ ëŒ€ë¶€ë¶„ì´ë©°, íˆìŠ¤í† ê·¸ë¨(íŒŒë€ìƒ‰)ì´ ì •ê·œë¶„í¬(ë¹¨ê°„ìƒ‰)ì™€ ë‹¤ë¦…ë‹ˆë‹¤.
    * **ê²°ë¡ :** ë°ì´í„°ê°€ í•œìª½ìœ¼ë¡œ ì ë ¤(Skewed) ìˆìœ¼ë¯€ë¡œ, **"ì¹˜ë£Œ(Transformation)"ê°€ í•„ìš”í•©ë‹ˆë‹¤.**

---

## 2ë‹¨ê³„: ğŸ’Š ìˆ˜ë™ ì¹˜ë£Œ (Manual Transformations)

* **ëª©ì :** "1ì°¨ ì§„ë£Œ". `log`, `sqrt`, `square` ë“± **"ì¼ë°˜ì ì¸ ë³€í™˜(ì¹˜ë£Œë²•)"** 7ê°€ì§€ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ "ì ë¦¼(Skewness)"ì„ ê°€ì¥ ì˜ ê³ ì¹˜ëŠ” ë³€í™˜ì„ ì°¾ìŠµë‹ˆë‹¤.
* **ë°©ë²•:** 7ê°€ì§€ ë³€í™˜ì„ ê°ê° ì ìš©í•œ ë’¤, `'Combined_Score': abs(skew) + abs(kurt) / 10` (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)ë¼ëŠ” ì‹¬ì‚¬ ì ìˆ˜ë¡œ 1ë“±ì„ ë½‘ìŠµë‹ˆë‹¤.

* **ì½”ë“œ ì˜ˆì‹œ:**
    ```python
    transformations = {
        'Original': lambda x: x,
        'Log': lambda x: np.log1p(x),
        'Square Root': lambda x: np.sqrt(x),
        # ... (Square, Reciprocal ë“±) ...
    }
    
    # 7ê°€ì§€ ë³€í™˜ì„ ëª¨ë‘ í…ŒìŠ¤íŠ¸í•˜ê³  Combined_Score ê³„ì‚°
    for feature in numeric_features:
        for trans_name, trans_func in transformations.items():
            # ... (try-exceptë¡œ skew, kurt ê³„ì‚°) ...
            transformation_results.append({ ... 'Combined_Score': ... })

    # ì ìˆ˜ê°€ ê°€ì¥ ë‚®ì€(ê°€ì¥ ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´) ë³€í™˜ì„ ì°¾ìŒ
    best_transformations = results_df.loc[results_df.groupby('Feature')['Combined_Score'].idxmin()]
    print(best_transformations.round(3))
    ```
* **ğŸ’¡ ì¸ì‚¬ì´íŠ¸:**
    * `age`ëŠ” `Cube Root`(ì„¸ì œê³±ê·¼)ì¼ ë•Œ, `tenure`ëŠ” `Square`(ì œê³±)ì¼ ë•Œ 'ì´ë¡ ì ìœ¼ë¡œ' ê°€ì¥ ëŒ€ì¹­ì ì¸ ëª¨ì–‘ì´ ë¨ì„ í™•ì¸í•©ë‹ˆë‹¤.

---

## 3ë‹¨ê³„: ğŸ”¬ ìë™ ìµœì í™” (Box-Cox and Yeo-Johnson)

* **ëª©ì :** "2ì°¨ ì •ë°€ ì§„ë£Œ". 2ë‹¨ê³„ì˜ 'ìˆ˜ë™' ë°©ì‹ì„ ë„˜ì–´, `Box-Cox`ì™€ `Yeo-Johnson`ì´ë¼ëŠ” **"ì „ë¬¸ ë³€í™˜ ì•Œê³ ë¦¬ì¦˜"** ì„ ì‚¬ìš©í•´ ìˆ˜í•™ì ìœ¼ë¡œ ìµœì í™”ëœ $\lambda$(ëŒë‹¤) ê°’ì„ **ìë™ìœ¼ë¡œ** ì°¾ìŠµë‹ˆë‹¤.
* **ë°©ë²•:**
    * **Box-Cox:** ì „í†µì ì¸ ìë™ ë³€í™˜ê¸°. (ë‹¨, 0 ë˜ëŠ” ìŒìˆ˜ ê°’ì´ ìˆìœ¼ë©´ ì‹¤íŒ¨)
    * **Yeo-Johnson:** Box-Coxì˜ ì—…ê·¸ë ˆì´ë“œ ë²„ì „. (0/ìŒìˆ˜ ê°’ë„ ì²˜ë¦¬ ê°€ëŠ¥)
* **ì½”ë“œ ì˜ˆì‹œ:**
    ```python
    fig, axes = plt.subplots(len(numeric_features), 3, ...)
    for idx, feature in enumerate(numeric_features):
        data = train_df[feature].values
        
        # 1. Original (íŒŒë€ìƒ‰)
        axes[idx, 0].hist(data, ...)
        
        # 2. Box-Cox (ì´ˆë¡ìƒ‰) - 0 ê°’ì´ ìˆìœ¼ë©´ ì‹¤íŒ¨ (Not applicable)
        if (data > 0).all():
            transformed_bc, lambda_bc = boxcox(data)
            axes[idx, 1].hist(transformed_bc, ...)
        
        # 3. Yeo-Johnson (ë¹¨ê°„ìƒ‰) - í•­ìƒ ì„±ê³µ
        pt = PowerTransformer(method='yeo-johnson')
        transformed_yj = pt.fit_transform(data.reshape(-1, 1)).ravel()
        axes[idx, 2].hist(transformed_yj, ...)
    ```
* **ê²°ê³¼ ë° í•´ì„:**
    
    * **ì‹œê°ì :** `payment_interval`ì²˜ëŸ¼ ì ë ¤ìˆë˜ ì›ë³¸(íŒŒë€ìƒ‰)ì´ `Yeo-Johnson`(ë¹¨ê°„ìƒ‰)ì„ í†µí•´ **ëŒ€ì¹­ì ì¸ ëª¨ì–‘**ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ "ì¹˜ë£Œ"ë˜ëŠ” ê²ƒì„ ëˆˆìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
    * **ì•ˆì •ì„±:** `Box-Cox`ëŠ” 0 ê°’ì´ í¬í•¨ëœ í”¼ì²˜ì—ì„œ ì‹¤íŒ¨("Not applicable")í•©ë‹ˆë‹¤.
    * **ğŸ’¡ ì¸ì‚¬ì´íŠ¸:** "ì ë¦¼(Skewness)"ì„ ì¡ëŠ” ê°€ì¥ ì•ˆì •ì ì´ê³  ê°•ë ¥í•œ "ì²˜ë°©ì „"ì€ **Yeo-Johnson** ë³€í™˜ì„ì„ í™•ì¸í•©ë‹ˆë‹¤.

---

## 4ë‹¨ê³„: ğŸš‘ ê°•ì œ ë³€í™˜ (Quantile Transformation)

* **ëª©ì :** "ìµœì¢… ìˆ˜ìˆ ". 3ë‹¨ê³„ `Yeo-Johnson`ìœ¼ë¡œë„ "ì ë¦¼"ì€ ì¡ì•˜ì§€ë§Œ "ìš¸í‰ë¶ˆí‰í•¨(ë‹¤ë´‰ì„±)"ì„ ì¡ì§€ ëª»í–ˆì„ ê²½ìš°, **"ì›ë³¸ì˜ ëª¨ì–‘ì„ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ì™„ë²½í•œ ë¶„í¬ë¥¼ ë¹šì–´ë‚´ëŠ”"** ê°€ì¥ ê³µê²©ì ì¸ ë³€í™˜ì…ë‹ˆë‹¤.
* **ë°©ë²•:** ë°ì´í„°ì˜ 'ê°’' ëŒ€ì‹  'ìˆœìœ„(Quantile)'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    1.  `output_distribution='normal'` (ì´ˆë¡ìƒ‰): ë°ì´í„°ë¥¼ **ê°•ì œë¡œ ì™„ë²½í•œ 'ì¢… ëª¨ì–‘'** ìœ¼ë¡œ ì¬ë°°ì¹˜í•©ë‹ˆë‹¤.
    2.  `output_distribution='uniform'` (ë¹¨ê°„ìƒ‰): ë°ì´í„°ë¥¼ **ê°•ì œë¡œ ì™„ë²½í•œ 'í‰ì§€'** ë¡œ ì¬ë°°ì¹˜í•©ë‹ˆë‹¤.
* **ì½”ë“œ ì˜ˆì‹œ:**
    ```python
    qt_normal = QuantileTransformer(output_distribution='normal')
    qt_uniform = QuantileTransformer(output_distribution='uniform')

    for idx, feature in enumerate(numeric_features[:3]):
        # 'normal' (ì¢… ëª¨ì–‘)ìœ¼ë¡œ ê°•ì œ ë³€í™˜
        transformed_normal = qt_normal.fit_transform(data).ravel()
        axes[0, idx].hist(transformed_normal, ...)
        
        # 'uniform' (í‰ì§€ ëª¨ì–‘)ìœ¼ë¡œ ê°•ì œ ë³€í™˜
        transformed_uniform = qt_uniform.fit_transform(data).ravel()
        axes[1, idx].hist(transformed_uniform, ...)
    ```
* **ğŸ’¡ ì¸ì‚¬ì´íŠ¸:**
    * ì´ ë³€í™˜ì€ ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆì§€ë§Œ, ì›ë³¸ ë°ì´í„°ì˜ ì˜ë¯¸(ì˜ˆ: 50ì„¸ì™€ 51ì„¸ì˜ 1ì‚´ ì°¨ì´)ë¥¼ **ì™„ì „íˆ íŒŒê´´**í•©ë‹ˆë‹¤.
    * "ì„±ëŠ¥"ì„ ìœ„í•´ "í•´ì„"ì„ í¬ìƒí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ìµœì¢… ì¹´ë“œì…ë‹ˆë‹¤.

---

## 5ë‹¨ê³„: ğŸ ìµœì¢… ì‹¤ì „ ê²€ì¦ (Performance Test)

* **ëª©ì :** "ê·¸ë˜ì„œ... 1~4ë²ˆì˜ ì´ë¡ ì ì¸ ë³€í™˜ ì¤‘, **'ì‹¤ì œë¡œ' ëª¨ë¸ ì„±ëŠ¥ì— ê°€ì¥ ë„ì›€ì´ ë˜ëŠ” ë³€í™˜ì€ ë¬´ì—‡ì¸ê°€?"**ì— ëŒ€í•œ **ìµœì¢… ë‹µ**ì„ ì°¾ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
* **ë°©ë²•:**
    1.  **ì „ì—­ í…ŒìŠ¤íŠ¸ (Global):** "ëª¨ë“  í”¼ì²˜ì— 'Original' ì ìš©" vs "ëª¨Së“  í”¼ì²˜ì— 'Yeo-Johnson' ì ìš©" ë“± 5ê°€ì§€ **ì „ëµ**ì„ `RandomForestClassifier`ë¡œ 5-ê²¹ êµì°¨ ê²€ì¦í•˜ì—¬ **F1-Score(ì„±ëŠ¥)** ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    2.  **ê°œë³„ í…ŒìŠ¤íŠ¸ (Individual):** ê° í”¼ì²˜ë³„ë¡œ 6ê°€ì§€ ë³€í™˜ì„ ì ìš©í•˜ì—¬, **íƒ€ê²Ÿ(`y`)ê³¼ì˜ ê´€ê³„(MI Score)** ê°€ ê°€ì¥ ë†’ì•„ì§€ëŠ” 'ìµœì ì˜ ë³€í™˜'ì„ ì°¾ìŠµë‹ˆë‹¤.
* **ì½”ë“œ ì˜ˆì‹œ (ì „ì—­ í…ŒìŠ¤íŠ¸):**
    ```python
    transformation_pipelines = {
        'Original': StandardScaler(),
        'Yeo-Johnson': PowerTransformer(method='yeo-johnson'),
        'Quantile-Normal': QuantileTransformer(output_distribution='normal'),
        # ...
    }
    
    for trans_name, transformer in transformation_pipelines.items():
        pipeline = Pipeline([('transform', transformer), ('model', RandomForestClassifier(...))])
        scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')
        # ... (ê²°ê³¼ ì €ì¥) ...

    performance_df.plot(kind='bar', y='Mean_F1', ...)
    ```
* **ì½”ë“œ ì˜ˆì‹œ (ê°œë³„ í…ŒìŠ¤íŠ¸):**
    ```python
    for feature in numeric_features:
        transforms_to_test = { ... }
        for trans_name, transformed_data in transforms_to_test.items():
            # íƒ€ê²Ÿ(y)ê³¼ì˜ MI Score ê³„ì‚°
            mi_score = mutual_info_classif(transformed_data, y, ...)[0]
            # ... (ìµœê³  ì ìˆ˜ ì°¾ê¸°) ...
    
    ax.bar(features, improvements, color=colors, ...)
    ```
